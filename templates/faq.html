{% extends "layout.html" %}

{% block meta %}
  {{ super() }}
  <meta property="og:title" content="Stanford Cable TV News Analyzer - FAQ" />
  <meta property="og:url" content="https://{{ host }}/faq" />
{% endblock %}

{% block head %}
{{ super() }}
<style>
  h3 {
    margin-bottom: 0.8em !important;
  }
</style>
{% endblock %}

{% block content %}
<div class="container">
  <h1>Frequently Asked Questions</h1>

  <hr>

  <ul>
      <li><a href="#mission">What are your goals in creating the Stanford Cable TV News Analyzer?</a></li>
      <li><a href="#facial_rec">Why does the Stanford Cable TV News Analyzer use automated face recognition technology?</a></li>
      {% if not hide_gender %}
      <li><a href="#gender">Why does the Stanford Cable TV News Analyzer automatically label faces with a binary gender label?</a>
      {% endif %}
      <li><a href="#race">Does the Stanford Cable TV News Analyzer attempt to identify the race of an individual?</a>
      <li><a href="#methods">What algorithms does the Stanford Cable TV News Analyzer use to annotate the dataset?</a>
      <li><a href="#remove_me">How do I request to be removed from the dataset?</a>
      <li><a href="#missing_face_id">I recognize an individual in the video, but the face in the video was not labeled with the individual’s name.  Why were they not identified?</a>
      <li><a href="#report_errors">How do I report errors in video annotations?</a>
      <li><a href="#update_rate">How quickly after a program airs is the Stanford Cable TV News Analyzer updated to reflect new content?</a>
      <li><a href="#known_confounds">What are some known confounds in the dataset?</a>
    </ul>

  <hr>

  <h3><a id="mission"></a>
    What are your goals in creating the Stanford Cable TV News Analyzer?
  </h3>

  <p>
    Our goal is to provide the public with computational tools that enable large-scale, data-driven analysis of the contents of cable TV news.  We believe the ability to quantitatively measure who is in the news and what is talked about will increase transparency about editorial decisions, serve as a powerful mechanism to identify forms of bias, and identify trends in an important information source that reaches millions of Americans each day.
  </p>

  <h3><a id="facial_rec"></a>
    Automated face recognition has been shown to have errors, bias, and has the potential to cause harm.  Why does the Stanford Cable TV News Analyzer use this technology?
  </h3>

  <p>
    The Stanford Cable TV News Analyzer uses automated face recognition technology provided by the <a href="https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html" target="_blank">Amazon Rekognition Celebrity Recognition API</a> to identify and compute the screen time of individuals on cable TV News.
    Face recognition, particularly when performed en masse on large image databases, is a <a href="https://www.nytimes.com/2020/06/09/technology/facial-recognition-software.html" target="_blank">controversial technology</a> because of its potential to cause harm due to <a href="http://gendershades.org/" target="_blank">errors and bias</a>, erode personal privacy, and <a href="https://www.nytimes.com/2020/01/12/technology/facial-recognition-police.html" target="_blank">misuse by governments and law enforcement</a>.  Due to these concerns the City of San Francisco has <a href="https://www.nytimes.com/2019/05/14/us/facial-recognition-ban-san-francisco.html" target="_blank">banned face recognition technology for law enforcement</a>, Amazon AWS <a href="https://blog.aboutamazon.com/policy/we-are-implementing-a-one-year-moratorium-on-police-use-of-rekognition" target="_blank">announced a moratorium</a> on police use of its face recognition services, and IBM recently announced that it is <a href="https://www.ibm.com/blogs/policy/facial-recognition-susset-racial-justice-reforms/" target="_blank">sunsetting its face identification services</a> entirely.
  </p>

  <p>
    At the same time, applying face recognition to large image databases plays a role in efforts such as <a href="https://www.wired.com/story/how-facial-recognition-fighting-child-sex-trafficking/" target="_blank">fighting human trafficking</a> and identify missing children.  We believe aiding public understanding of the share of screen time given to specific public figures on cable TV news programs is a new application of face recognition technology for which the potential for harm is low.
  </p>

  <p>
    <b>Individual privacy concerns.</b> The Stanford Cable TV News Analyzer only applies face recognition to publicly-aired broadcast cable TV news video. Additionally, our database contains only individuals identified by the <a href="https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html">Amazon Rekognition Celebrity Recognition API</a>, which identifies only <i>public figures</i>. (Amazon does not disclose their definition of "public figure".)  Additionally, the Stanford Cable TV News Analyzer only permits screen time queries for individuals that have received at least <i>ten hours of screen time</i> as of December 31, 2021 (according to the Celebrity Recognition API results).
    This is a total of {{ n_people }} individuals.  The full set of individuals identified in our dataset is given on our <a href="/data">dataset page</a>.  Individuals that wish to be removed from the dataset should email us at <a href="mailto: tvnews-project@stanford.edu?subject=RemoveMe">tvnews-project@stanford.edu</a>.
  </p>

  <p>
    <b>Accuracy and bias concerns.</b>  Automated facial recognition will have errors, and studies of other face recognition services have demonstrated accuracy biases by <a href="http://gendershades.org/" target="_blank">gender and race</a>.  It is not feasible to validate the accuracy of all face identifications in our dataset, but we provide results from a number of validation efforts on our <a href="/methodology">methodology page</a>.  We also point users to recent <a href="https://arxiv.org/abs/2001.00964" target="_blank">studies of accuracy</a> of the Amazon Rekognition service.  Also, to help users build trust in the accuracy of their own query results and to identify face identification errors, the Stanford Cable TV News Viewer provides the ability to directly view the video clips selected by queries.
  </p>

  <hr>

  {% if not hide_gender %}
  <h3><a id="gender"></a>
    Gender is a non-binary quantity and cannot be assessed from a person’s appearance alone.  Why does the Stanford Cable TV News Analyzer automatically label faces with a binary gender label?
  </h3>
  <p>
    The Stanford Cable TV News Analyzer uses computer vision to make a binary assessment of an individual’s <i>presented gender</i> based on the appearance of their face (see our <a href="/methodology">methodology page</a> for the algorithms used to do this). <a href="https://www.glaad.org/reference/transgender">Gender presentation</a> (also referred to as gender expression) reflects an individual’s external expression of their gender (through cues such as facial features, makeup, hairstyle, and clothing), which may be different from both their gender identity and/or their birth sex.  When an individual's presented gender differs from their actual gender identity, algorithmic attempts to infer gender identity from facial appearance will fail.
  </p>
  <p>
    We recognize that treating an individual’s gender as a binary quantity, as well as assessing gender solely from an individual's appearance, is a grossly simplified treatment of a complex topic. Further, we recognize that perpetuating the notion of binary gender can cause harm to non-binary individuals.
    However, we believe that binary classification of presented gender still provides useful insights into the presentation of cable TV news, and illuminates important biases in the screen time given to male- and female-presenting groups.  We believe these benefits justify the inclusion of binary gender labels in the tool.
  </p>

  <p>
    <b>Mitigating potential for harm.</b> Automatic gender recognition can result in automated misgendering, which can be distressing and harmful, especially to transgender individuals. Prior studies of existing automatic gender recognition systems have found that error rates are higher for <a href="http://gendershades.org/" target="_blank">dark-skinned individuals</a> and <a href="https://dl.acm.org/doi/10.1145/3359246" target="_blank">transgender individuals</a>. We provide statistics on the accuracy of our gender classifier on our <a href="/methodology">methodology page</a>.
  <p>

  <p>
    To mitigate potential harm due to automated misgendering, the Stanford Cable TV News Analyzer does not present gender labels in the user interface unless a user specifically "opts-in" to see these annotations by using <b>"tag=male"</b> or <b>"tag=female"</b> predicates in their search query.
  </p>

  <p>
    We also provide the ability to report misgendered individuals via <a href="mailto: tvnews-project@stanford.edu?subject=Report Gender">tvnews-project@stanford.edu</a>.
  </p>

  <hr>
  {% endif %}

  <h3><a id="race"></a>
    Does the Stanford Cable TV News Analyzer attempt to identify the race of an individual?
  </h3>

  <p>
    The Stanford Cable TV News Analyzer does not attempt to determine the race of individuals.  We are unaware of any computational model that can accurately estimate an individual’s race from their appearance. In the future, it may be possible to use external data sources to link public figure identities to the individual’s self-reported race.  Such approaches would enable a new set of queries that could assist studies of representation in cable TV news that concern the subject of race.
  </p>

  <hr>

  <h3><a id="methods"></a>
    I am concerned about possible sources of error and bias in my query results. What algorithms does the Stanford Cable TV News Analyzer use to annotate the dataset?
  </h3>
  <p>
    We document our data labeling algorithms as well as provide an assessment of their accuracy on the <a href="/methodology">methodology page</a>.
  </p>

  <hr>

  <h3><a id="remove_me"></a>
    How do I request to be removed from the dataset?
  </h3>
  <p>
    To request removal from the dataset, please email <a href="mailto: tvnews-project@stanford.edu?subject=Remove Me">tvnews-project@stanford.edu</a>.
  </p>

  <hr>

  <h3><a id="missing_face_id"></a>
    I recognize an individual in the video, but the face in the video was not labeled with the individual’s name.  Why were they not identified?
  </h3>
  <p>
    The Stanford Cable TV News Analyzer uses the <a href="https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html" target="_blank">Amazon’s Celebrity Recognition</a> service to identify faces. This service is only designed to identify celebrities and public individuals.  In addition, the site only displays the names of individuals that have appeared on screen for at least ten hours by December 31, 2021 (according to the celebrity recognition detections).  Individuals not identified by <a href="https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html" target="_blank">Amazon’s Celebrity Recognition</a> service, or individuals who are identified, but only briefly appear on screen, will not be shown on the site.
  </p>

  <p>
    Note that regardless of the success of face identification, text transcripts can always be queried for an individual’s name even if the individual’s face is not identified on screen.
  </p>

  <hr>

  <h3><a id="report_errors"></a>
    {% if not hide_gender %}
    I see a missed face detection, a misidentified face, or misgendered individual in the dataset.
    {% else %}
    I see a missed face detection or a misidentified face in the dataset.
    {% endif %}
    How do I report the error?
  </h3>
  <p>
    Due to the scale of our dataset, we will not be able to correct all labeling errors. However, we welcome comments and feedback via <a href="mailto: tvnews-project@stanford.edu?subject=Feedback">tvnews-project@stanford.edu</a>.
  </p>

  <hr>

  <h3><a id="update_rate"></a>
    How quickly after a program airs is the site updated to reflect new content?
  </h3>

  <p>
    The Internet Archive makes video data available to the Stanford Cable TV News Analyzer after a 24-hour delay.
    Because of this delay, as well the processing time of video analysis, new results appear on the Stanford Cable TV News Analyzer approximately 24-36 hours after a program's original air time.  Please see our <a href="/methodology">methodology</a> page for more detail.
  </p>

  <hr>

  <h3><a id="known_confounds"></a>
    What are some known confounds in the dataset?
  </h3>

  <p>
    This list is by no means comprehensive, but is instead intended as a set of examples for the kinds of confounds that one should be aware of when using this tool. We highly recommend using the video playback functions provided by the tool to validate the results of any query.

    <ul>
      <li><b>Variation in caption spellings.</b>
        The canonical spelling of a word or phrase may not be consistently reflected in the captions. For example, the word "Obamacare" often appears as "Obama care" (two separate words). "Email" is also often written as "E mail".</li>
      <li><b>Changes in the face identification models.</b>
        This tool relies on the <a href="https://docs.aws.amazon.com/rekognition/latest/dg/celebrities.html" target="_blank">Amazon Rekognition Celebrity Recognition API</a> for face identification. Amazon's API may change without notice. For example, Laura Ingraham, a Fox News host, is no longer detected after September 2021. Similarly, if new individuals are added and begin receiving screen time, this is not a guarantee that they were not present in the historical data when face identification was performed.</li>
    </ul>

    Our <a href="/paper">research paper</a> provides additional details on other patterns in the dataset.
  </p>

</div>
{% endblock %}
